{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Campaign Intelligence Starter Setup Script\n",
    "\n",
    "## Project Setup using Notebook\n",
    "\n",
    "### *Pre-requisites:*\n",
    "\n",
    "1. You have *conda/miniconda* installed. If not, please go to the following [docs.conda.io](https://docs.conda.io/projects/miniconda/en/latest/) or you can also use venv to create a virtual environment for this application.\n",
    "2. Role with access to install NativeApps from marketplace like DATA_HARMONIZATION_DEV\n",
    "3. Any environment to run notebooks like VS code, Jupyterlab, etc.\n",
    "\n",
    "### Update config files\n",
    "The proyect contains some config file that should be updated with your own environment variables \n",
    "1. `app_config.json`\n",
    "\n",
    "2. `connection_config.json`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup local environment (only the first time running the notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#### We have disabled the results of this block using %%capture but feel free to uncomment to debug\n",
    "\n",
    "#! conda create --name data_harmonization python=3.8 -y\n",
    "#! conda install -c anaconda ipykernel -y\n",
    "#! python -m ipykernel install --user --name=data_harmonization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#! python3 -m venv data_harmonization \n",
    "#! source data_harmonization/bin/activate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the default Kernel to  <span style=\"color:yellow\">data_harmonization</span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install package dependencies to kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.app_config import get_app_config\n",
    "\n",
    "app_config = get_app_config(is_local_installation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "def executeStatement(statement):\n",
    "    resultConn = subprocess.run(['snow', 'sql', '-q', statement])\n",
    "    return resultConn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear resources (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "subprocess.run(['snow', 'app', 'teardown'])\n",
    "print(executeStatement(f\"DROP DATABASE IF EXISTS FIVETRAN_CONNECTOR_DEMO\"))\n",
    "print(executeStatement(f\"DROP DATABASE IF EXISTS OMNATA_CONNECTOR_DEMO\"))\n",
    "print(executeStatement(f\"DROP DATABASE IF EXISTS DATA_QUALITY_NOTEBOOKS\"))\n",
    "print(executeStatement(f\"DROP DATABASE IF EXISTS LLM_DEMO\"))\n",
    "print(executeStatement(f\"DROP WAREHOUSE IF EXISTS {app_config['dynamic_table_warehouse']}\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Snowflake "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Native App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!snow app run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload sample notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_app_package = f\"CREATE DATABASE IF NOT EXISTS {app_config['data_quality_database']};\"\n",
    "\n",
    "create_app_schema = f\"CREATE SCHEMA IF NOT EXISTS {app_config['data_quality_database']}.{app_config['schema']};\"\n",
    "\n",
    "create_app_stage = f\"CREATE STAGE IF NOT EXISTS {app_config['data_quality_database']}.{app_config['schema']}.{app_config['nativeapp_stage']} \\\n",
    "    DIRECTORY = (ENABLE = TRUE) \\\n",
    "    COMMENT = 'Used for holding source code of native app.';\"\n",
    "\n",
    "create_wh = f\"CREATE OR REPLACE WAREHOUSE {app_config['dynamic_table_warehouse']} WITH WAREHOUSE_SIZE= MEDIUM;\"\n",
    "\n",
    "print(executeStatement(create_app_package))\n",
    "print(executeStatement(create_app_schema))\n",
    "print(executeStatement(create_app_stage))\n",
    "print(executeStatement(create_wh))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.upload_files import upload_files_stage\n",
    "\n",
    "\n",
    "database = app_config['data_quality_database']\n",
    "schema = app_config['schema']\n",
    "stage = app_config['nativeapp_stage']\n",
    "native_app_dir = './notebooks'\n",
    "\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# Directories to ignore\n",
    "dirs_ignore = ['/streamlit/frontend', 'pycache', 'tests']\n",
    "is_ignore = lambda path: len(list(filter(lambda ignore: ignore in path, dirs_ignore))) > 0\n",
    "\n",
    "\n",
    "def upload_files_stage(database: str, schema: str, stage: str, app_dir: str) -> None:\n",
    "    for path, currentDirectory, files in os.walk(app_dir):\n",
    "        for file in files:\n",
    "            dir = app_dir.replace(\"./\",\"\")\n",
    "            if not file.startswith('.') and not is_ignore(path):\n",
    "                # build the relative paths to the file\n",
    "                local_file = os.path.join(path, file)\n",
    "                replace_path = os.path.join('.',dir)\n",
    "\n",
    "                # build the path to where the file will be staged\n",
    "                stage_dir = path.replace(replace_path,'')\n",
    "                print(f'{local_file} => @{stage}{stage_dir}')\n",
    "                stage_location = f'@{database}.{schema}.{stage}/{stage_dir}'\n",
    "                print(local_file)\n",
    "                print(stage_location)\n",
    "                subprocess.run(['snow', 'stage', 'copy', local_file, stage_location])\n",
    "                executeStatement(f'alter stage {database}.{schema}.{stage} refresh; ')\n",
    "\n",
    "upload_files_stage(database,schema,stage, native_app_dir)\n",
    "\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appDeployed = subprocess.run (['snow', 'app', 'version', 'list'], stdout=subprocess.PIPE)\n",
    "result = appDeployed.stdout.decode(\"utf-8\")\n",
    "print(result)\n",
    "appPackageName = result[result.index(\"Marketing_Data_Foundation\"): result.index(\"\\n\")]\n",
    "appName = appPackageName.replace(\"_pkg_\",\"_\")\n",
    "print(appName)\n",
    "\n",
    "create_sample_db_facebook = \"CREATE DATABASE IF NOT EXISTS FIVETRAN_CONNECTOR_DEMO\"\n",
    "create_sample_schema_facebook = \"CREATE SCHEMA IF NOT EXISTS FIVETRAN_CONNECTOR_DEMO.FACEBOOK_RAW\"\n",
    "create_sample_schema_linkedin =\"CREATE SCHEMA IF NOT EXISTS OMNATA_CONNECTOR_DEMO.LINKEDIN_RAW\"\n",
    "create_samle_db_linkedin = \"CREATE DATABASE IF NOT EXISTS OMNATA_CONNECTOR_DEMO\"\n",
    "create_llm_demo_db = \"CREATE DATABASE IF NOT EXISTS LLM_DEMO\"\n",
    "create_llm_schema = \"CREATE SCHEMA IF NOT EXISTS LLM_DEMO.DEMO\"\n",
    "grant_db_facebook = f\"GRANT USAGE ON DATABASE FIVETRAN_CONNECTOR_DEMO TO APPLICATION {appName}\"\n",
    "grant_db_linkedin = f\"GRANT USAGE ON DATABASE OMNATA_CONNECTOR_DEMO TO APPLICATION {appName}\"\n",
    "grant_schema_facebook = f\"GRANT USAGE ON SCHEMA FIVETRAN_CONNECTOR_DEMO.FACEBOOK_RAW TO APPLICATION {appName}\"\n",
    "grant_schema_linkedin = f\"GRANT USAGE ON SCHEMA OMNATA_CONNECTOR_DEMO.LINKEDIN_RAW TO APPLICATION {appName}\"\n",
    "print(executeStatement(create_sample_db_facebook))\n",
    "print(executeStatement(create_samle_db_linkedin))\n",
    "print(executeStatement(create_sample_schema_facebook))\n",
    "print(executeStatement(create_sample_schema_linkedin))\n",
    "print(executeStatement(grant_db_facebook))\n",
    "print(executeStatement(grant_db_linkedin))\n",
    "print(executeStatement(grant_schema_facebook))\n",
    "print(executeStatement(grant_schema_linkedin))\n",
    "print(executeStatement(create_llm_demo_db))\n",
    "print(executeStatement(create_llm_schema))\n",
    "\n",
    "executeStatement(f\"\"\"CREATE OR REPLACE NOTEBOOK  LLM_DEMO.DEMO.DATA_QUALITY_DEMO_1\n",
    "    FROM '@data_quality_notebooks.APP.CODE_STG'\n",
    "    MAIN_FILE = 'data_quality_demo_1.ipynb'\n",
    "    QUERY_WAREHOUSE = '{app_config['warehouse']}';\"\"\")\n",
    "\n",
    "executeStatement(\"ALTER NOTEBOOK LLM_DEMO.DEMO.DATA_QUALITY_DEMO_1 ADD LIVE VERSION FROM LAST\")\n",
    "\n",
    "\n",
    "executeStatement(f\"\"\"CREATE OR REPLACE NOTEBOOK LLM_DEMO.DEMO.DATA_QUALITY_DEMO_2\n",
    "    FROM '@data_quality_notebooks.APP.CODE_STG'\n",
    "    MAIN_FILE = 'data_quality_demo_2.ipynb'\n",
    "    QUERY_WAREHOUSE = '{app_config['warehouse']}';\"\"\")\n",
    "\n",
    "executeStatement(\"ALTER NOTEBOOK LLM_DEMO.DEMO.DATA_QUALITY_DEMO_2 ADD LIVE VERSION FROM LAST\")\n",
    "\n",
    "executeStatement(f\"\"\"CREATE OR REPLACE NOTEBOOK LLM_DEMO.DEMO.DATA_QUALITY_DEMO_3\n",
    "    FROM '@data_quality_notebooks.APP.CODE_STG'\n",
    "    MAIN_FILE = 'data_quality_demo_3.ipynb'\n",
    "    QUERY_WAREHOUSE = '{app_config['warehouse']}';\"\"\")\n",
    "\n",
    "executeStatement(\"ALTER NOTEBOOK LLM_DEMO.DEMO.DATA_QUALITY_DEMO_3 ADD LIVE VERSION FROM LAST\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Demo Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from snowflake.snowpark.functions import parse_json\n",
    "\n",
    "import os\n",
    "data_dir = \"./data\"\n",
    "print(\"Loading Tables\")\n",
    "\n",
    "\n",
    "database = app_config['data_quality_database']\n",
    "schema = app_config['schema']\n",
    "stage = app_config['nativeapp_stage']\n",
    "stage_location = f'@{database}.{schema}.{stage}/sample_data'\n",
    "\n",
    "csv_file_format = f\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT DATA_QUALITY_NOTEBOOKS.APP.csv_format\n",
    "  TYPE = csv\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "  PARSE_HEADER = true;\n",
    "\"\"\"\n",
    "executeStatement(''.join(csv_file_format.splitlines()))\n",
    "\n",
    "for path, currentDirectory, files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        if file in [\".DS_Store\"]:\n",
    "            continue\n",
    "        else:\n",
    "            database = appName.upper()\n",
    "            file_path = os.path.join(path, file)\n",
    "            print(\"Loading File:\" + file_path)            \n",
    "            table_name = file.split(\".\")[0].upper()\n",
    "            if table_name == \"DIM_PLATFORM\":\n",
    "                schema = \"CAMPAIGN_INTELLIGENCE_COMBINED\"\n",
    "            else:\n",
    "                schema = file_path.replace(data_dir+\"/\",\"\").replace(\"/\"+file,\"\").upper()\n",
    "                schema = schema.lstrip(\"/\")\n",
    "            if len(schema.split(\"/\")) > 1:\n",
    "                database, schema = [i.upper() for i in schema.split(\"/\")]\n",
    "            fileName = os.path.basename(file_path)\n",
    "            subprocess.run(['snow', 'stage', 'copy', file_path, stage_location])\n",
    "            createTable = f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {database}.{schema}.{table_name}\n",
    "                USING TEMPLATE (\n",
    "                    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n",
    "                    FROM TABLE(\n",
    "                        INFER_SCHEMA(\n",
    "                        LOCATION=>'{stage_location}/{fileName}',\n",
    "                        FILE_FORMAT => 'DATA_QUALITY_NOTEBOOKS.APP.csv_format'\n",
    "                        )\n",
    "                    ));\n",
    "            \"\"\"\n",
    "            executeStatement(''.join(createTable.splitlines()))\n",
    "            copyTo = f\"\"\"COPY INTO {database}.{schema}.{table_name} FROM {stage_location}/{fileName} FILE_FORMAT = (FORMAT_NAME= 'DATA_QUALITY_NOTEBOOKS.APP.csv_format')  MATCH_BY_COLUMN_NAME=\"CASE_INSENSITIVE\"; \"\"\"\n",
    "            executeStatement(copyTo)\n",
    "            changeTracking = f\"ALTER TABLE {database}.{schema}.{table_name} SET CHANGE_TRACKING = TRUE;\"\n",
    "            executeStatement(changeTracking)\n",
    "  \n",
    "\n",
    "grant_select_db_linkedin = f\"GRANT SELECT ON ALL TABLES IN SCHEMA OMNATA_CONNECTOR_DEMO.LINKEDIN_RAW TO APPLICATION {appName}\"\n",
    "grant_select_db_facebook = f\"GRANT SELECT ON ALL TABLES IN SCHEMA FIVETRAN_CONNECTOR_DEMO.FACEBOOK_RAW TO APPLICATION {appName}\"\n",
    "print(executeStatement(grant_select_db_facebook))\n",
    "print(executeStatement(grant_select_db_linkedin))\n",
    "print('Success')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dynamic Table Stored Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_name = appName\n",
    "\n",
    "## Create Dynamic Table Procedure\n",
    "create_procedure_commands = f\"\"\"\n",
    "CREATE OR REPLACE PROCEDURE {application_name}.USER_SETTINGS.CREATE_DYNAMIC_TABLE(dynamic_table_name VARCHAR, query VARCHAR)\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = 3.8\n",
    "HANDLER = 'creator'\n",
    "PACKAGES = ('snowflake-snowpark-python')\n",
    "AS\n",
    "$$\n",
    "def creator(session, dynamic_table_name, query):\n",
    "    session.sql(query).collect()\n",
    "$$;\n",
    "\"\"\"\n",
    "grant_procedure_usage = f\"GRANT USAGE ON PROCEDURE {application_name}.USER_SETTINGS.CREATE_DYNAMIC_TABLE(VARCHAR, VARCHAR) TO APPLICATION {application_name};\"\n",
    "executeStatement(create_procedure_commands)\n",
    "executeStatement(grant_procedure_usage)\n",
    "\n",
    "\n",
    "# Create grants procedure\n",
    "create_procedure_commands = f\"\"\"\n",
    "CREATE OR REPLACE PROCEDURE {application_name}.USER_SETTINGS.GRANTER(application_name VARCHAR, schema VARCHAR, tables VARIANT)\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = 3.8\n",
    "HANDLER = 'granter'\n",
    "PACKAGES = ('snowflake-snowpark-python')\n",
    "AS\n",
    "$$\n",
    "\n",
    "def granter(session, application_name, schema, tables):\n",
    "    for table_name in tables:\n",
    "        session.sql(f'GRANT SELECT ON TABLE {{application_name}}.{{schema}}.{{table_name}} TO APPLICATION {{application_name}}').collect()\n",
    "$$;\n",
    "\"\"\"\n",
    "grant_procedure_usage = f\"GRANT USAGE ON PROCEDURE {application_name}.USER_SETTINGS.GRANTER(VARCHAR, VARCHAR, VARIANT) TO APPLICATION {application_name};\"\n",
    "executeStatement(create_procedure_commands)\n",
    "executeStatement(grant_procedure_usage)\n",
    "\n",
    "\n",
    "\n",
    "## Create Show Tables Procedure\n",
    "get_tables_query = f\"\"\"\n",
    "CREATE OR REPLACE PROCEDURE {application_name}.USER_SETTINGS.SHOW_TABLES(database_name VARCHAR, schema_name VARCHAR)\n",
    "RETURNS VARIANT\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = 3.8\n",
    "HANDLER = 'get_non_change_tracking_tables'\n",
    "PACKAGES = ('snowflake-snowpark-python')\n",
    "AS\n",
    "$$\n",
    "from snowflake.snowpark import dataframe as df\n",
    "def get_non_change_tracking_tables(session, database_name, schema_name):\n",
    "    tables = session.sql(f\"SHOW TABLES IN SCHEMA {{database_name}}.{{schema_name}}\").select('\"name\"').where(df.col('\"change_tracking\"') == \"OFF\").collect()\n",
    "    table_names = [row[0] if not any(c.islower() for c in row[0]) else f'\"{{row[0]}}\"' for row in tables]\n",
    "    return table_names\n",
    "$$;\n",
    "\"\"\"\n",
    "grant_procedure_usage = f\"GRANT USAGE ON PROCEDURE {application_name}.USER_SETTINGS.SHOW_TABLES(VARCHAR, VARCHAR) TO APPLICATION {application_name};\"\n",
    "executeStatement(get_tables_query)\n",
    "executeStatement(grant_procedure_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload assistant semantic model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.upload_files import upload_files_stage\n",
    "from scripts.update_file_variables import file_replace\n",
    "\n",
    "database = appName\n",
    "schema = 'LLM'\n",
    "semantic_models_stage = 'SEMANTIC_MODEL'\n",
    "llm_config_stage = 'CONFIGURATION'\n",
    "\n",
    "replace_map = {\n",
    "    \"<target_database>\": appName\n",
    "}\n",
    "file_replace('./assistant/semantic_models/UnifiedMarketingModel_CAMPAIGN_PERF.yaml', replace_map)\n",
    "\n",
    "subprocess.run(['snow', 'stage', 'copy', './assistant/config/assistant_config.yaml', f\"@{database}.{schema}.{llm_config_stage}\"])\n",
    "subprocess.run(['snow', 'stage', 'copy', './assistant/semantic_models/UnifiedMarketingModel_CAMPAIGN_PERF.yaml', f\"@{database}.{schema}.{semantic_models_stage}\"])\n",
    "\n",
    "print('Success')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_harmonization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
